['conv2_spmm_data_new_format_2/full-00.std']
['conv2_spmm_data_new_format_2/chunk-cosim.std']
============================= test session starts ==============================
platform linux -- Python 3.6.8, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 -- /work/shared/users/staff/zz546/venvs/master-pytorch/bin/python
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/scratch/users/zz546/hb-pytorch/hammerblade/scripts/tests/conv2_spmm/conv2_spmm_xeon_2_0/.hypothesis/examples')
rootdir: /scratch/users/zz546/pytorch-cosim/hb-pytorch/hammerblade/torch/tests, inifile: pytest.ini
plugins: hypothesis-5.8.3
collecting ... collected 1 item

../../../../../../pytorch-cosim/hb-pytorch/hammerblade/torch/tests/profiler/test_lenet5_sparseconv2_lowerspmm_profiler.py::test_lenet5_sparse01_conv2 Emulating CUDALite...
Emulation barrier init'ed with 1 threads
PyTorch configed with 1 * 1 HB device
HB startup config kernel applied
 ATen profiler collecting ...
at top level kernel at::Tensor at::CPUType::{anonymous}::unfold(const at::Tensor&, int64_t, int64_t, int64_t)
should I redispatch? 1/0
at top level kernel at::Tensor at::CPUType::{anonymous}::unfold(const at::Tensor&, int64_t, int64_t, int64_t)
should I redispatch? 1/0
at top level kernel at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)
should I redispatch? 1/0
redispatching...
@#ACTUALS#@__self;[1, 20, 8, 8, 5, 5]<|>
#TOP_LEVEL_FUNC#__at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t);13917
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@BSG_API_CALL@__free;17
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@BSG_API_CALL@__free<|>@TRIM@;0
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@;3883
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef);3828
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>);3708
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::TypeDefault::empty_like(const at::Tensor&, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>);28
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::TypeDefault::empty_like(const at::Tensor&, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::CPUType::{anonymous}::empty(c10::IntArrayRef, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>);12
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool);3638
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub();3608
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef);77
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::view(const at::Tensor&, c10::IntArrayRef);61
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::alias(const at::Tensor&);6
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor& at::CPUType::{anonymous}::set_(at::Tensor&, c10::Storage, int64_t, c10::IntArrayRef, c10::IntArrayRef);19
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::CPUType::{anonymous}::view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor& at::CPUType::{anonymous}::set_(at::Tensor&, c10::Storage, int64_t, c10::IntArrayRef, c10::IntArrayRef)<|>at::Tensor& at::native::legacy::cpu::_th_set_(at::Tensor&, c10::Storage, int64_t, c10::IntArrayRef, c10::IntArrayRef);5
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@CPU_LOG@<|>int64_t at::TypeDefault::size(const at::Tensor&, int64_t);4
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@;9826
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef);9771
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>);9591
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::TypeDefault::empty_like(const at::Tensor&, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>);49
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::TypeDefault::empty_like(const at::Tensor&, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::HammerBladeType::{anonymous}::empty(c10::IntArrayRef, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>);31
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::TypeDefault::empty_like(const at::Tensor&, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::HammerBladeType::{anonymous}::empty(c10::IntArrayRef, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>)<|>@BSG_API_CALL@__malloc;9
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::TypeDefault::empty_like(const at::Tensor&, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>)<|>at::Tensor at::HammerBladeType::{anonymous}::empty(c10::IntArrayRef, const c10::TensorOptions&, c10::optional<c10::MemoryFormat>)<|>@BSG_API_CALL@__malloc<|>@TRIM@;0
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool);9507
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub();9475
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub()<|>@BSG_API_CALL@__free;61
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub()<|>@BSG_API_CALL@__free<|>@TRIM@;0
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub()<|>@BSG_API_CALL@__malloc;46
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub()<|>@BSG_API_CALL@__malloc<|>@TRIM@;0
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub()<|>@BSG_API_CALL@__memcpy;47
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub()<|>@BSG_API_CALL@__memcpy<|>@TRIM@;0
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub()<|>@OFFLOAD_KERNEL@__tensorlib_copy_Float_to_Float;9101
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::clone(const at::Tensor&, c10::optional<c10::MemoryFormat>)<|>at::Tensor& at::TypeDefault::copy_(at::Tensor&, const at::Tensor&, bool)<|>at::native::copy_stub::copy_stub()<|>@OFFLOAD_KERNEL@__tensorlib_copy_Float_to_Float<|>@TRIM@;0
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef);84
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::view(const at::Tensor&, c10::IntArrayRef);60
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::alias(const at::Tensor&);8
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>at::Tensor at::TypeDefault::reshape(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::TypeDefault::_unsafe_view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor at::HammerBladeType::{anonymous}::view(const at::Tensor&, c10::IntArrayRef)<|>at::Tensor& at::HammerBladeType::{anonymous}::set_(at::Tensor&, c10::Storage, int64_t, c10::IntArrayRef, c10::IntArrayRef);7
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>@HB_LOG@<|>int64_t at::TypeDefault::size(const at::Tensor&, int64_t);2
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>at::Tensor at::CPUType::{anonymous}::llcopy(const at::Tensor&);76
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>at::Tensor at::CPUType::{anonymous}::llcopy(const at::Tensor&)<|>@BSG_API_CALL@__dma;15
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>at::Tensor at::CPUType::{anonymous}::llcopy(const at::Tensor&)<|>@BSG_API_CALL@__dma<|>@TRIM@;0
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>at::Tensor at::CPUType::{anonymous}::llcopy(const at::Tensor&)<|>@BSG_API_CALL@__malloc;7
at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)<|>at::Tensor at::CPUType::{anonymous}::llcopy(const at::Tensor&)<|>@BSG_API_CALL@__malloc<|>@TRIM@;0

#TOP_LEVEL_FUNC_END#__at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)
at top level kernel at::Tensor at::TypeDefault::transpose(const at::Tensor&, int64_t, int64_t)
should I redispatch? 1/0
at top level kernel at::Tensor at::TypeDefault::transpose(const at::Tensor&, int64_t, int64_t)
should I redispatch? 1/0
at top level kernel at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)
should I redispatch? 1/0
at top level kernel at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)
should I redispatch? 1/0
at top level kernel at::Tensor at::TypeDefault::t(const at::Tensor&)
should I redispatch? 1/0
at top level kernel at::Tensor at::TypeDefault::contiguous(const at::Tensor&, c10::MemoryFormat)
should I redispatch? 1/0
at top level kernel at::Tensor at::SparseCPUType::{anonymous}::_sparse_mm(const at::Tensor&, const at::Tensor&)
should I redispatch? 1/0
PASSED

============================== 1 passed in 0.72s ===============================
+ COSIM_PYTHON_EXE=/scratch/users/zz546/bsg_bladerunner//bsg_replicant/testbenches/pytorch/test_loader
+ [[ ! -f /scratch/users/zz546/bsg_bladerunner//bsg_replicant/testbenches/pytorch/test_loader ]]
+ eval '/scratch/users/zz546/bsg_bladerunner//bsg_replicant/testbenches/pytorch/test_loader +ntb_random_seed_automatic +c_args="-m' pytest -vs '/scratch/users/zz546/pytorch-cosim/hb-pytorch/hammerblade/torch/tests/profiler/test_lenet5_sparseconv2_lowerspmm_profiler.py"' '| grep -v ": instantiating\|\[.*_PROFILER\]"'
++ /scratch/users/zz546/bsg_bladerunner//bsg_replicant/testbenches/pytorch/test_loader +ntb_random_seed_automatic '+c_args=-m pytest -vs /scratch/users/zz546/pytorch-cosim/hb-pytorch/hammerblade/torch/tests/profiler/test_lenet5_sparseconv2_lowerspmm_profiler.py'
++ grep -v ': instantiating\|\[.*_PROFILER\]'
Chronologic VCS simulator copyright 1991-2018
Contains Synopsys proprietary information.
Compiler version O-2018.09-SP2_Full64; Runtime version O-2018.09-SP2_Full64;  Jul 22 16:56 2020
NOTE: automatic random seed used: 1391183586
==================== BSG MACHINE SETTINGS: ====================
[INFO][TESTBENCH] BSG_MACHINE_GLOBAL_X                 =          16
[INFO][TESTBENCH] BSG_MACHINE_GLOBAL_Y                 =           8
[INFO][TESTBENCH] BSG_MACHINE_VCACHE_SET               =          64
[INFO][TESTBENCH] BSG_MACHINE_VCACHE_WAY               =           8
[INFO][TESTBENCH] BSG_MACHINE_VCACHE_BLOCK_SIZE_WORDS  =          16
[INFO][TESTBENCH] BSG_MACHINE_MAX_EPA_WIDTH            =          28
[INFO][TESTBENCH] BSG_MACHINE_MEM_CFG                  = e_infinite_mem
tb.card.fpga.CL.core_clk_gen with cycle_time_p      400000
## ----------------------------------------------------------------
## MANYCORE HETERO TYPE CONFIGUREATIONS
## ----------------------------------------------------------------
## 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
## 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
## 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
## 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
## 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
## 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
## 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
## 0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,
## ----------------------------------------------------------------
WARNING: Behavioral models for independent clock FIFO configurations do not model synchronization delays. The behavioral models are functionally correct, and will represent the behavior of the configured FIFO. See the FIFO Generator User Guide for more information.
WARNING: Behavioral models for independent clock FIFO configurations do not model synchronization delays. The behavioral models are functionally correct, and will represent the behavior of the configured FIFO. See the FIFO Generator User Guide for more information.
WARNING: Behavioral models for independent clock FIFO configurations do not model synchronization delays. The behavioral models are functionally correct, and will represent the behavior of the configured FIFO. See the FIFO Generator User Guide for more information.
WARNING: Behavioral models for independent clock FIFO configurations do not model synchronization delays. The behavioral models are functionally correct, and will represent the behavior of the configured FIFO. See the FIFO Generator User Guide for more information.
WARNING: Behavioral models for independent clock FIFO configurations do not model synchronization delays. The behavioral models are functionally correct, and will represent the behavior of the configured FIFO. See the FIFO Generator User Guide for more information.
BSG INFO: test_pytorch Regression Test 
============================= test session starts ==============================
platform linux -- Python 3.6.8, pytest-5.4.1, py-1.8.1, pluggy-0.13.1 -- /work/shared/users/staff/zz546/venvs/sparse-pytorch/bin/python3
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/scratch/users/zz546/hb-pytorch/hammerblade/scripts/tests/conv2_spmm/conv2_spmm_hb_2/.hypothesis/examples')
rootdir: /scratch/users/zz546/pytorch-cosim/hb-pytorch/hammerblade/torch/tests, inifile: pytest.ini
plugins: hypothesis-5.10.4
collecting ... collected 1 item

../../../../../../pytorch-cosim/hb-pytorch/hammerblade/torch/tests/profiler/test_lenet5_sparseconv2_lowerspmm_profiler.py::test_lenet5_sparse01_conv2 "/scratch/users/zz546/bsg_bladerunner/bsg_manycore/v/bsg_manycore_endpoint_standard.v", 346: tb.card.fpga.CL.mcl_to_axil.mc_ep_to_fifos.epsd.genblk2.unnamed$$_0: started at 194168000ps failed at 194168000ps
	Offending '((out_credits_o === 'x) || (out_credits_o > 5'b0))'
## out of remote store credits(= 0) x,y= 0, 1 displaying only once (tb.card.fpga.CL.mcl_to_axil.mc_ep_to_fifos.epsd)
##   (this may be a performance problem; or normal behavior)
[INFO][RX] Unfreezing tile t=199606000000, x= 0, y= 2
[INFO][RX] Unfreezing tile t=199611200000, x= 1, y= 2
[INFO][RX] Unfreezing tile t=199617200000, x= 2, y= 2
[INFO][RX] Unfreezing tile t=199624000000, x= 3, y= 2
[INFO][RX] Unfreezing tile t=199631600000, x= 4, y= 2
[INFO][RX] Unfreezing tile t=199640000000, x= 5, y= 2
[INFO][RX] Unfreezing tile t=199649200000, x= 6, y= 2
[INFO][RX] Unfreezing tile t=199659200000, x= 7, y= 2
[INFO][RX] Unfreezing tile t=199670000000, x= 8, y= 2
[INFO][RX] Unfreezing tile t=199681600000, x= 9, y= 2
[INFO][RX] Unfreezing tile t=199694000000, x=10, y= 2
[INFO][RX] Unfreezing tile t=199707200000, x=11, y= 2
[INFO][RX] Unfreezing tile t=199721200000, x=12, y= 2
[INFO][RX] Unfreezing tile t=199736000000, x=13, y= 2
[INFO][RX] Unfreezing tile t=199751600000, x=14, y= 2
[INFO][RX] Unfreezing tile t=199768000000, x=15, y= 2
[INFO][RX] Unfreezing tile t=199776800000, x= 0, y= 3
[INFO][RX] Unfreezing tile t=199782800000, x= 1, y= 3
[INFO][RX] Unfreezing tile t=199789600000, x= 2, y= 3
[INFO][RX] Unfreezing tile t=199797200000, x= 3, y= 3
[INFO][RX] Unfreezing tile t=199805600000, x= 4, y= 3
[INFO][RX] Unfreezing tile t=199814800000, x= 5, y= 3
[INFO][RX] Unfreezing tile t=199824800000, x= 6, y= 3
[INFO][RX] Unfreezing tile t=199835600000, x= 7, y= 3
[INFO][RX] Unfreezing tile t=199847200000, x= 8, y= 3
[INFO][RX] Unfreezing tile t=199859600000, x= 9, y= 3
[INFO][RX] Unfreezing tile t=199872800000, x=10, y= 3
[INFO][RX] Unfreezing tile t=199886800000, x=11, y= 3
[INFO][RX] Unfreezing tile t=199901600000, x=12, y= 3
[INFO][RX] Unfreezing tile t=199917200000, x=13, y= 3
[INFO][RX] Unfreezing tile t=199933600000, x=14, y= 3
[INFO][RX] Unfreezing tile t=199950800000, x=15, y= 3
[INFO][RX] Unfreezing tile t=199960400000, x= 0, y= 4
[INFO][RX] Unfreezing tile t=199967200000, x= 1, y= 4
[INFO][RX] Unfreezing tile t=199974800000, x= 2, y= 4
[INFO][RX] Unfreezing tile t=199983200000, x= 3, y= 4
[INFO][RX] Unfreezing tile t=199992400000, x= 4, y= 4
[INFO][RX] Unfreezing tile t=200002400000, x= 5, y= 4
[INFO][RX] Unfreezing tile t=200013200000, x= 6, y= 4
[INFO][RX] Unfreezing tile t=200024800000, x= 7, y= 4
[INFO][RX] Unfreezing tile t=200037200000, x= 8, y= 4
[INFO][RX] Unfreezing tile t=200050400000, x= 9, y= 4
[INFO][RX] Unfreezing tile t=200064400000, x=10, y= 4
[INFO][RX] Unfreezing tile t=200079200000, x=11, y= 4
[INFO][RX] Unfreezing tile t=200094800000, x=12, y= 4
[INFO][RX] Unfreezing tile t=200111200000, x=13, y= 4
[INFO][RX] Unfreezing tile t=200128400000, x=14, y= 4
[INFO][RX] Unfreezing tile t=200146400000, x=15, y= 4
[INFO][RX] Unfreezing tile t=200156800000, x= 0, y= 5
[INFO][RX] Unfreezing tile t=200164400000, x= 1, y= 5
[INFO][RX] Unfreezing tile t=200172800000, x= 2, y= 5
[INFO][RX] Unfreezing tile t=200182000000, x= 3, y= 5
[INFO][RX] Unfreezing tile t=200192000000, x= 4, y= 5
[INFO][RX] Unfreezing tile t=200202800000, x= 5, y= 5
[INFO][RX] Unfreezing tile t=200214400000, x= 6, y= 5
[INFO][RX] Unfreezing tile t=200226800000, x= 7, y= 5
[INFO][RX] Unfreezing tile t=200240000000, x= 8, y= 5
[INFO][RX] Unfreezing tile t=200254000000, x= 9, y= 5
[INFO][RX] Unfreezing tile t=200268800000, x=10, y= 5
[INFO][RX] Unfreezing tile t=200284400000, x=11, y= 5
[INFO][RX] Unfreezing tile t=200300800000, x=12, y= 5
[INFO][RX] Unfreezing tile t=200318000000, x=13, y= 5
[INFO][RX] Unfreezing tile t=200336000000, x=14, y= 5
[INFO][RX] Unfreezing tile t=200354800000, x=15, y= 5
[INFO][RX] Unfreezing tile t=200366000000, x= 0, y= 6
[INFO][RX] Unfreezing tile t=200374400000, x= 1, y= 6
[INFO][RX] Unfreezing tile t=200383600000, x= 2, y= 6
[INFO][RX] Unfreezing tile t=200393600000, x= 3, y= 6
[INFO][RX] Unfreezing tile t=200404400000, x= 4, y= 6
[INFO][RX] Unfreezing tile t=200416000000, x= 5, y= 6
[INFO][RX] Unfreezing tile t=200428400000, x= 6, y= 6
[INFO][RX] Unfreezing tile t=200441600000, x= 7, y= 6
[INFO][RX] Unfreezing tile t=200455600000, x= 8, y= 6
[INFO][RX] Unfreezing tile t=200470400000, x= 9, y= 6
[INFO][RX] Unfreezing tile t=200486000000, x=10, y= 6
[INFO][RX] Unfreezing tile t=200502400000, x=11, y= 6
[INFO][RX] Unfreezing tile t=200519600000, x=12, y= 6
[INFO][RX] Unfreezing tile t=200537600000, x=13, y= 6
[INFO][RX] Unfreezing tile t=200556400000, x=14, y= 6
[INFPyTorch configed with 16 * 8 HB device
HB startup config kernel applied
 ATen profiler collecting ...
at top level kernel at::Tensor at::CPUType::{anonymous}::unfold(const at::Tensor&, int64_t, int64_t, int64_t)
should I redispatch? 1/0
at top level kernel at::Tensor at::CPUType::{anonymous}::unfold(const at::Tensor&, int64_t, int64_t, int64_t)
should I redispatch? 1/0
at top level kernel at::Tensor at::TypeDefault::flatten(const at::Tensor&, int64_t, int64_t)
should I redispatch? 1/0
redispatching...
@#ACTUALS#@__self;[1, 20, 8, 8, 5, 5]<|>
Traceback (most recent call last):
  File "/scratch/users/zz546/hb-pytorch/hammerblade/scripts/compare_aten_op.py", line 358, in <module>
    ops.append(compare(args.full[i], args.chunk[i], args.manycore_stats, args.fancy))
  File "/scratch/users/zz546/hb-pytorch/hammerblade/scripts/compare_aten_op.py", line 300, in compare
    chunk_actuals, chunk_raw_stack = std_parser.parse(f_chunk.read())
  File "/scratch/users/zz546/hb-pytorch/hammerblade/scripts/std_parser.py", line 34, in parse
    assert stack_start is not None
AssertionError
